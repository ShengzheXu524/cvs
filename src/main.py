#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è€ƒç ”è‹±è¯­çœŸé¢˜æ–‡æ¡£å¤„ç†ä¸»ç¨‹åº
è´Ÿè´£æµç¨‹ç¼–æ’ä¸æ§åˆ¶
"""

import os
import sys
import json
import logging
import argparse
import time
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path
import re

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.openrouter_handler import OpenRouterHandler
from src.content_analyzer import ContentAnalyzer
from src.model_config import get_model
from src.data_organizer import DataOrganizer
from src.csv_generator import CSVGenerator
from src.sentence_splitter import split_sentences
from src.docx_reader import DocxReader

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("main")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def process_file(input_file, model_name=None, output_dir="test_results", save_debug=False, gen_csv=True):
    """
    å¤„ç†æŒ‡å®šçš„æ–‡æ¡£æ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤æ¨¡å‹
        output_dir: è¾“å‡ºç›®å½•
        save_debug: æ˜¯å¦ä¿å­˜è°ƒè¯•ä¿¡æ¯
        gen_csv: æ˜¯å¦ç”ŸæˆCSVæ–‡ä»¶
    
    Returns:
        dict: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬:
            - success: å¤„ç†æ˜¯å¦æˆåŠŸ
            - json_path: ä¿å­˜çš„JSONç»“æœè·¯å¾„
            - csv_path: ä¿å­˜çš„CSVç»“æœè·¯å¾„(å¦‚æœç”Ÿæˆäº†)
            - analysis_time: åˆ†æè€—æ—¶
    """
    logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        file_extension = Path(input_file).suffix.lower()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "analysis"), exist_ok=True)
        if save_debug:
            os.makedirs(os.path.join(output_dir, "debug"), exist_ok=True)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–å†…å®¹
        if file_extension == '.docx':
            logger.info("æ£€æµ‹åˆ°Wordæ–‡æ¡£ï¼Œä½¿ç”¨DocxReaderè¯»å–")
            docx_reader = DocxReader()
            document_text = docx_reader.read_file(input_file)
            
            # å¦‚æœéœ€è¦è°ƒè¯•ï¼Œä¿å­˜æå–çš„æ–‡æœ¬
            if save_debug:
                # æå–å¹´ä»½ä½œä¸ºç›®å½•åï¼ˆä»æ–‡ä»¶åä¸­æå–ï¼‰
                file_name = os.path.basename(input_file)
                year_match = re.search(r'(\d{4})', file_name)
                year = year_match.group(1) if year_match else "unknown"
                
                # ç¡®ä¿å¹´ä»½ç›®å½•å­˜åœ¨
                year_dir = os.path.join(output_dir, year)
                os.makedirs(year_dir, exist_ok=True)
                os.makedirs(os.path.join(year_dir, "debug"), exist_ok=True)
                
                # ä¿å­˜æå–çš„æ–‡æœ¬
                extracted_text_path = os.path.join(year_dir, "debug", f"{os.path.splitext(file_name)[0]}_extracted.txt")
                with open(extracted_text_path, 'w', encoding='utf-8') as f:
                    f.write(document_text)
                logger.info(f"æå–çš„æ–‡æœ¬å·²ä¿å­˜åˆ°: {extracted_text_path}")
        else:
            # é»˜è®¤å½“ä½œæ–‡æœ¬æ–‡ä»¶å¤„ç†
            logger.info("æ–‡æœ¬æ–‡ä»¶ï¼Œç›´æ¥è¯»å–å†…å®¹")
            with open(input_file, 'r', encoding='utf-8') as f:
                document_text = f.read()
        
        # åˆå§‹åŒ–APIå¤„ç†å™¨
        api_handler = OpenRouterHandler(model=model_name)
        
        # åˆå§‹åŒ–å†…å®¹åˆ†æå™¨
        content_analyzer = ContentAnalyzer(api_handler=api_handler)
        
        # æå–æ•°æ®
        extract_start_time = time.time()
        result = content_analyzer.extract_data(document_text, save_debug=save_debug, output_dir=output_dir)
        extract_time = time.time() - extract_start_time
        logger.info(f"æ•°æ®æå–è€—æ—¶: {extract_time:.2f} ç§’")
        
        # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        safe_model_name = api_handler.model.replace('/', '_')
        
        # æå–å¹´ä»½ä½œä¸ºç›®å½•å
        year_match = re.search(r'(\d{4})', base_name)
        year = year_match.group(1) if year_match else None
        if year:
            # ä½¿ç”¨å¹´ä»½ä½œä¸ºå­ç›®å½•
            year_dir = os.path.join(output_dir, year)
            os.makedirs(year_dir, exist_ok=True)
            os.makedirs(os.path.join(year_dir, "analysis"), exist_ok=True)
            if save_debug:
                os.makedirs(os.path.join(year_dir, "debug"), exist_ok=True)
            output_json = os.path.join(year_dir, "analysis", f"{base_name}_{safe_model_name}_{timestamp}.json")
        else:
            # æ²¡æœ‰å¹´ä»½æ—¶ä½¿ç”¨åŸç›®å½•
            output_json = os.path.join(output_dir, "analysis", f"{base_name}_{safe_model_name}_{timestamp}.json")
        
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump({
                "model": api_handler.model,
                "input_file": os.path.basename(input_file),
                "analysis_time_seconds": extract_time,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "result": result
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSONç»“æœå·²ä¿å­˜åˆ°: {output_json}")
        
        # æ£€æŸ¥æ˜¯å¦å®Œæ•´æå–äº†52é“é¢˜
        questions = result.get("questions", [])
        if len(questions) < 52:
            logger.warning(f"è­¦å‘Šï¼šåªæå–äº† {len(questions)} é“é¢˜ç›®ï¼Œä¸æ˜¯å®Œæ•´çš„52é“é¢˜")
            logger.info(f"é¢˜å·åˆ—è¡¨: {sorted([q.get('number', 0) for q in questions])}")
        else:
            logger.info(f"æˆåŠŸæå–äº†æ‰€æœ‰ {len(questions)} é“é¢˜ç›®")
        
        # å¦‚æœéœ€è¦ç”ŸæˆCSV
        csv_path = None
        if gen_csv:
            try:
                logger.info("å¼€å§‹ç”ŸæˆCSVæ–‡ä»¶...")
                csv_start_time = time.time()
                
                # åˆå§‹åŒ–æ•°æ®ç»„ç»‡å™¨å’ŒCSVç”Ÿæˆå™¨
                data_organizer = DataOrganizer()
                csv_generator = CSVGenerator()
                
                # ç»„ç»‡æ•°æ®
                organized_data = data_organizer.organize_data(result)
                
                # ç¡®ä¿æ•°æ®é›†å®Œæ•´
                complete_data = data_organizer.ensure_complete_dataset(organized_data)
                
                # åº”ç”¨å¥å­æ‹†åˆ†
                processed_data = data_organizer.apply_sentence_splitter(complete_data, split_sentences)
                
                # ä¿å­˜ç»„ç»‡åçš„æ•°æ®
                if year:
                    organized_json = os.path.join(year_dir, "analysis", f"organized_data_{timestamp}.json")
                    csv_dir = year_dir
                else:
                    organized_json = os.path.join(output_dir, "analysis", f"organized_data_{timestamp}.json")
                    csv_dir = output_dir
                
                with open(organized_json, 'w', encoding='utf-8') as f:
                    json.dump(processed_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ç»„ç»‡åçš„æ•°æ®å·²ä¿å­˜åˆ°: {organized_json}")
                
                # ç¡®å®šCSVæ–‡ä»¶å
                year_from_result = result.get("metadata", {}).get("year", year if year else "æœªçŸ¥å¹´ä»½")
                exam_type = result.get("metadata", {}).get("exam_type", "æœªçŸ¥ç±»å‹")
                csv_filename = f"{year_from_result}{exam_type}.csv"
                csv_path = os.path.join(csv_dir, csv_filename)
                
                # ç”ŸæˆCSVæ–‡ä»¶
                csv_success = csv_generator.generate_csv(processed_data, csv_path)
                csv_time = time.time() - csv_start_time
                
                if csv_success:
                    logger.info(f"CSVæ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: {csv_path}ï¼Œè€—æ—¶: {csv_time:.2f} ç§’")
                else:
                    logger.error(f"CSVæ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼Œè€—æ—¶: {csv_time:.2f} ç§’")
                    csv_path = None
                
            except Exception as e:
                logger.error(f"ç”ŸæˆCSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                csv_path = None
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        total_time = time.time() - start_time
        logger.info(f"æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        return {
            "success": True,
            "json_path": output_json,
            "csv_path": csv_path,
            "analysis_time": total_time
        }
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return {
            "success": False,
            "json_path": None,
            "csv_path": None,
            "analysis_time": time.time() - start_time
        }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è€ƒç ”è‹±è¯­çœŸé¢˜æ–‡æ¡£å¤„ç†å·¥å…·")
    parser.add_argument('input_file', help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒtxtå’Œdocxæ ¼å¼")
    parser.add_argument('--model', help="ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œç•™ç©ºåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­DEFAULT_MODELæŒ‡å®šçš„æ¨¡å‹")
    parser.add_argument('--output-dir', default="test_results", help="è¾“å‡ºç›®å½•ï¼Œä¼šè‡ªåŠ¨æ ¹æ®å¹´ä»½åˆ›å»ºå­ç›®å½•")
    parser.add_argument('--debug', action='store_true', help="ä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼ŒåŒ…æ‹¬APIå“åº”å’Œä¸­é—´ç»“æœ")
    parser.add_argument('--no-csv', action='store_true', help="ä¸ç”ŸæˆCSVæ–‡ä»¶ï¼Œä»…ç”ŸæˆJSONç»“æœ")
    parser.add_argument('--year', help="æŒ‡å®šå¹´ä»½ï¼Œç”¨äºåˆ›å»ºè¾“å‡ºå­ç›®å½•ï¼Œé»˜è®¤ä»æ–‡ä»¶åä¸­æå–")
    
    # æ·»åŠ å¸®åŠ©æ–‡æœ¬
    parser.epilog = """
è¯¦ç»†è¯´æ˜:
  æœ¬å·¥å…·æ”¯æŒå¤„ç†txtå’Œdocxæ ¼å¼çš„è€ƒç ”è‹±è¯­çœŸé¢˜ï¼Œä¼šè‡ªåŠ¨æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©å¤„ç†æ–¹å¼:
  - å¯¹äºdocxæ–‡ä»¶ï¼Œä¼šè‡ªåŠ¨æå–æ–‡æœ¬å¹¶å¤„ç†
  - å¯¹äºtxtæ–‡ä»¶ï¼Œç›´æ¥è¯»å–å†…å®¹å¤„ç†
  - ç»“æœä¼šæ ¹æ®å¹´ä»½è‡ªåŠ¨ä¿å­˜åœ¨å¯¹åº”çš„å­ç›®å½•ä¸­
  - è¶…è¿‡3000å­—ç¬¦çš„é•¿æ–‡æ¡£ä¼šè‡ªåŠ¨ä½¿ç”¨åˆ†æ®µå¤„ç†ï¼Œæé«˜APIè°ƒç”¨æ•ˆç‡
  - ä½¿ç”¨--debugå‚æ•°å¯ä»¥ä¿å­˜APIå“åº”å’Œä¸­é—´å¤„ç†ç»“æœï¼Œä¾¿äºåˆ†æå’Œè°ƒè¯•
  - é»˜è®¤ä¼šåŒæ—¶ç”ŸæˆJSONå’ŒCSVæ ¼å¼çš„ç»“æœæ–‡ä»¶ï¼Œä½¿ç”¨--no-csvå¯ä»¥ç¦ç”¨CSVç”Ÿæˆ
  
ç¤ºä¾‹:
  python src/main.py input.txt --model google/gemini-2.5-flash-preview --debug
  python src/main.py 2023å¹´è€ƒç ”è‹±è¯­çœŸé¢˜.docx --output-dir custom_results
  python src/main.py input.txt --year 2022 --no-csv  # æ‰‹åŠ¨æŒ‡å®šå¹´ä»½ï¼Œä¸ç”ŸæˆCSVæ–‡ä»¶
"""
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return 1
    
    # å¦‚æœç”¨æˆ·æŒ‡å®šäº†å¹´ä»½ï¼Œä¿®æ”¹è¾“å‡ºç›®å½•
    output_dir = args.output_dir
    if args.year:
        output_dir = os.path.join(args.output_dir, args.year)
    
    # å¤„ç†æ–‡ä»¶
    result = process_file(
        args.input_file, 
        model_name=args.model, 
        output_dir=output_dir,
        save_debug=args.debug,
        gen_csv=not args.no_csv
    )
    
    # è¾“å‡ºå¤„ç†ç»“æœæ‘˜è¦
    if result["success"]:
        logger.info("âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ")
        logger.info(f"ğŸ“Š JSONç»“æœ: {result['json_path']}")
        if result["csv_path"]:
            logger.info(f"ğŸ“ CSVç»“æœ: {result['csv_path']}")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {result['analysis_time']:.2f} ç§’")
        return 0
    else:
        logger.error("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 