#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è€ƒç ”è‹±è¯­çœŸé¢˜é€šç”¨å¤„ç†è„šæœ¬
æ”¯æŒå¤„ç†ä»»æ„å¹´ä»½çš„çœŸé¢˜ï¼Œè‡ªåŠ¨é€‚é…docxå’Œtxtæ ¼å¼
"""

import os
import sys
import logging
import argparse
import time
import re
from pathlib import Path
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("process_exam")

# å°†å½“å‰ç›®å½•æ·»åŠ åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ•°æ®å¤„ç†å™¨
from src.data_processor import DataProcessor
from src.docx_reader import DocxReader

def process_exam(input_file, output_dir=None, model_name=None, save_debug=False, year=None):
    """
    å¤„ç†è€ƒç ”è‹±è¯­çœŸé¢˜æ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ ¹æ®å¹´ä»½åˆ›å»º
        model_name: æ¨¡å‹åç§°
        save_debug: æ˜¯å¦ä¿å­˜è°ƒè¯•ä¿¡æ¯
        year: æ‰‹åŠ¨æŒ‡å®šå¹´ä»½ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ä»æ–‡ä»¶åæå–
    
    Returns:
        tuple: (æ˜¯å¦æˆåŠŸ, CSVæ–‡ä»¶è·¯å¾„, å¤„ç†æ—¶é—´)
    """
    start_time = time.time()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return False, None, 0
    
    # å°è¯•ä»æ–‡ä»¶åä¸­æå–å¹´ä»½
    if year is None:
        file_name = os.path.basename(input_file)
        year_match = re.search(r'(\d{4})', file_name)
        year = year_match.group(1) if year_match else "unknown"
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œåˆ™ä½¿ç”¨å¹´ä»½ä½œä¸ºå­ç›®å½•
    if output_dir is None:
        output_dir = os.path.join("test_results", year)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "analysis"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "debug"), exist_ok=True)
    
    logger.info("=" * 50)
    logger.info(f"å¼€å§‹å¤„ç†è€ƒç ”è‹±è¯­çœŸé¢˜: {input_file}")
    logger.info(f"ç›®æ ‡å¹´ä»½: {year}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    file_extension = Path(input_file).suffix.lower()
    if file_extension == '.docx':
        logger.info("æ£€æµ‹åˆ°Wordæ–‡æ¡£ï¼Œå°†è‡ªåŠ¨æå–æ–‡æœ¬")
    elif file_extension == '.txt':
        logger.info("æ£€æµ‹åˆ°æ–‡æœ¬æ–‡æ¡£ï¼Œç›´æ¥å¤„ç†")
    else:
        logger.warning(f"æœªçŸ¥çš„æ–‡ä»¶ç±»å‹: {file_extension}ï¼Œå°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶å¤„ç†")
    
    try:
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        processor = DataProcessor(model_name=model_name)
        
        # å¤„ç†æ–‡æ¡£
        success, csv_path, process_time = processor.process_document(
            document_path=input_file,
            output_dir=output_dir,
            save_debug=save_debug
        )
        
        # å¤„ç†ç»“æœ
        if success:
            logger.info(f"âœ… {year}å¹´è€ƒç ”è‹±è¯­çœŸé¢˜å¤„ç†æˆåŠŸï¼")
            logger.info(f"ğŸ“Š CSVæ–‡ä»¶å·²ç”Ÿæˆ: {csv_path}")
            logger.info(f"â±ï¸ å¤„ç†è€—æ—¶: {process_time:.2f}ç§’")
        else:
            logger.error(f"âŒ {year}å¹´è€ƒç ”è‹±è¯­çœŸé¢˜å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {process_time:.2f}ç§’")
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœè·¯å¾„
        if success and csv_path:
            analysis_dir = os.path.join(output_dir, "analysis")
            analysis_files = os.listdir(analysis_dir)
            json_files = [f for f in analysis_files if f.endswith('.json')]
            
            if json_files:
                latest_json = sorted(json_files)[-1]
                logger.info(f"ğŸ“„ æœ€æ–°åˆ†ææ•°æ®: {os.path.join(analysis_dir, latest_json)}")
        
        return success, csv_path, process_time
    
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}", exc_info=True)
        total_time = time.time() - start_time
        return False, None, total_time

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è€ƒç ”è‹±è¯­çœŸé¢˜é€šç”¨å¤„ç†å·¥å…·")
    parser.add_argument('input_file', help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒdocxå’Œtxtæ ¼å¼")
    parser.add_argument('--output-dir', '-o', help="è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºtest_results/å¹´ä»½")
    parser.add_argument('--model', '-m', help="ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®çš„æ¨¡å‹")
    parser.add_argument('--debug', '-d', action='store_true', help="ä¿å­˜è°ƒè¯•ä¿¡æ¯")
    parser.add_argument('--year', '-y', help="æ‰‹åŠ¨æŒ‡å®šå¹´ä»½ï¼Œé»˜è®¤ä»æ–‡ä»¶åæå–")
    
    # ä½¿ç”¨è¯´æ˜
    parser.epilog = """
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†docxæ–‡ä»¶ (è‡ªåŠ¨ä»æ–‡ä»¶åæå–å¹´ä»½)
  python process_exam.py 2022å¹´è€ƒç ”è‹±è¯­(ä¸€)çœŸé¢˜.docx --debug
  
  # å¤„ç†txtæ–‡ä»¶å¹¶æ‰‹åŠ¨æŒ‡å®šå¹´ä»½
  python process_exam.py kaoyan_english.txt --year 2021
  
  # æŒ‡å®šè¾“å‡ºç›®å½•å’Œä½¿ç”¨ç‰¹å®šæ¨¡å‹
  python process_exam.py 2023å¹´è€ƒç ”è‹±è¯­.docx --output-dir my_results/2023 --model anthropic/claude-3-5-sonnet
    """
    
    args = parser.parse_args()
    
    # å¤„ç†æ–‡ä»¶
    success, csv_path, process_time = process_exam(
        input_file=args.input_file,
        output_dir=args.output_dir,
        model_name=args.model,
        save_debug=args.debug,
        year=args.year
    )
    
    # è¿”å›çŠ¶æ€ç 
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main()) 